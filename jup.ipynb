{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "dataset = ir_datasets.load(\"antique/test\")\n",
    "dataset2 = ir_datasets.load(\"beir/quora/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import save_npz\n",
    "from scipy.sparse import load_npz\n",
    "from flask import Flask, request, jsonify, render_template, json\n",
    "import gzip\n",
    "from scipy.sparse import csr_matrix, load_npz\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import enchant\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "documents_keys = []\n",
    "\n",
    "documents2 = []\n",
    "documents_keys2 = []\n",
    "\n",
    "d = enchant.DictWithPWL(\"en_US\", \"my_pwl.txt\")\n",
    "\n",
    "def assign_data_set_to_documents():\n",
    "    for i, doc in enumerate(dataset.docs_iter()):\n",
    "        documents.append(doc[1])\n",
    "        documents_keys.append(doc[0])\n",
    "        \n",
    "def assign_data_set_to_documents2():\n",
    "    for i, doc in enumerate(dataset2.docs_iter()):\n",
    "        documents2.append(doc[1])\n",
    "        documents_keys2.append(doc[0])\n",
    "        \n",
    "def remove_whitespace(text):\n",
    "\treturn \" \".join(text.split()) \n",
    "\n",
    "def remove_stopwords(words):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_text = [word for word in words if word not in stop_words]\n",
    "    return filtered_text\n",
    "\n",
    "def stem_words(words):\n",
    "\tstems = [stemmer.stem(word) for word in words]\n",
    "\treturn stems\n",
    "\n",
    "def stem_words(words):\n",
    "\tstems = [stemmer.stem(word) for word in words]\n",
    "\treturn stems\n",
    "\n",
    "def remove_lemma(words):\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word, tag in tagged_words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        lemmatized_words.append(lemma)\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "def is_date_or_time(word):\n",
    "    try:\n",
    "        parser.parse(word)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    # Remove punctuation\n",
    "    for word in words:\n",
    "        if  word.isalpha() or is_date_or_time(word):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def text_lowercase(text):\n",
    "\treturn text.lower()\n",
    "\n",
    "def format_dates(document):\n",
    "    # Define a regular expression pattern to match different date formats\n",
    "    date_pattern = re.compile(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}')\n",
    "    time_pattern = re.compile(r'\\d{1,2}:\\d{2}(:\\d{2})?\\s*(AM|PM|am|pm)?')\n",
    "    # Define the desired output date format\n",
    "    output_format = \"%Y-%m-%d\"\n",
    "    output_time_format = \"%I:%M %p\"\n",
    "    # Replace the dates in the document with the desired output format\n",
    "    for match in date_pattern.findall(document):\n",
    "        try:\n",
    "            date_obj = parser.parse(match)\n",
    "            standard_date_format = date_obj.strftime(output_format)\n",
    "            document = document.replace(match, standard_date_format)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    try:\n",
    "        document = time_pattern.sub(lambda match: datetime.strptime(match.group(), '%I:%M %p' if match.group(2) else '%H:%M').strftime(output_time_format), document)\n",
    "    except ValueError:\n",
    "        print(\"\")\n",
    "    return document\n",
    "\n",
    "def process_shortcuts(document):\n",
    "    matches = []\n",
    "    shortcut_dict = {\n",
    "    'p.p.s':'post postscript',\n",
    "    'u.s.a': 'united states of america',\n",
    "    'a.k.a': 'also known as',\n",
    "    'm.a.d': 'Mutually Assured Destruction',\n",
    "    'a.b.b': 'Asea Brown Boveri',\n",
    "    's.c.o': 'Santa Cruz Operation',\n",
    "    'e.t.c': 'etcetera',\n",
    "    'm.i.t': 'Massachusetts Institute of Technology',\n",
    "    'v.i.p': 'very important person',\n",
    "    'us':'united states of america',\n",
    "    'u.s.':'united states of america',\n",
    "    'usa':'united states of america',\n",
    "    'cobol':'common business oriented language',\n",
    "    'rpm':'red hat package manager',\n",
    "    'ap':'associated press',\n",
    "    'gpa':'grade point average',\n",
    "    'npr':'national public radio',\n",
    "    'fema':'federal emergency',\n",
    "    'crt':'cathode ray tube',\n",
    "    'gm':'grandmaster',\n",
    "    'fps':'frames per second',\n",
    "    'pc':'personal computer',\n",
    "    'pms':'premenstrual syndrome',\n",
    "    'cia':'central intelligence agency',\n",
    "    'aids':'acquired immune deficiency syndrome',\n",
    "    'it\\'s':'it is',\n",
    "    'you\\'ve':'you have',\n",
    "    'what\\'s':'what is',\n",
    "    'that\\'s':'that is',\n",
    "    'who\\'s':'who is',\n",
    "    'don\\'t':'do not',\n",
    "    'haven\\'t':'have not',\n",
    "    'there\\'s':'there is',\n",
    "    'i\\'d':'i would',\n",
    "    'it\\'ll':'it will',\n",
    "    'i\\'m':'i am',\n",
    "    'here\\'s':'here is',\n",
    "    'you\\'ll':'you will',\n",
    "    'cant\\'t':'can not',\n",
    "    'didn\\'t':'did not',\n",
    "    'hadn\\'t':'had not',\n",
    "    'kv':'kilovolt',\n",
    "    'cc':'cubic centimeter',\n",
    "    'aoa':'american osteopathic association',\n",
    "    'rbi':'reserve bank',\n",
    "    'pls':'please',\n",
    "    'dvd':'digital versatile disc',\n",
    "    'bdu':'boise state university',\n",
    "    'dvd':'digital versatile disc',\n",
    "    'mac':'macintosh',\n",
    "    'tv':'television',\n",
    "    'cs':'computer science',\n",
    "    'cse':'computer science engineering',\n",
    "    'iit':'indian institutes of technology',\n",
    "    'uk':'united kingdom',\n",
    "    'eee':'electrical and electronics engineering',\n",
    "    'ca':'california',\n",
    "    'etc':'etcetera',\n",
    "    'ip':'internet protocol',\n",
    "    'bjp':'bharatiya janata party',\n",
    "    'gdp':' gross domestic product',\n",
    "    'un':'unitednations',\n",
    "    'ctc':'cost to company',\n",
    "    'atm':'automated teller machine',\n",
    "    'pvt':'private',\n",
    "    'iim':'indian institutes of management'\n",
    "    \n",
    "    }\n",
    "    shortcut_pattern1 = re.compile(r'[A-Za-z]\\.[A-Za-z]\\.[A-Za-z]*')\n",
    "    shortcut_pattern2 = re.compile(r'\\b[A-Za-z]{2,3}\\b')\n",
    "    shortcut_pattern3 = re.compile(r'\\w+\\'\\w+')\n",
    "    \n",
    "    matches.append(shortcut_pattern1.findall(document)) \n",
    "    matches.append(shortcut_pattern2.findall(document))\n",
    "    matches.append(shortcut_pattern3.findall(document))\n",
    "    \n",
    "    for arr in matches:\n",
    "        for match in arr:\n",
    "            if match in shortcut_dict:\n",
    "                document = document.replace(match, shortcut_dict[match])     \n",
    "            \n",
    "    return document\n",
    "\n",
    "\n",
    "def text_processing(choosed_document):\n",
    "    docs_array = []\n",
    "    for i, document in enumerate(choosed_document):\n",
    "        document = remove_whitespace(document)\n",
    "        document = text_lowercase(document)\n",
    "        document = process_shortcuts(document)\n",
    "        document = format_dates(document)\n",
    "        words = word_tokenize(document)\n",
    "        words = remove_punctuation(words)\n",
    "        words = remove_stopwords(words)\n",
    "        words = remove_lemma(words)\n",
    "        words = stem_words(words)\n",
    "        docs_array.append(words)\n",
    "    return docs_array\n",
    "\n",
    "def make_inverted_index(docs_array):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for i, doc in enumerate(docs_array):\n",
    "        for token in doc:\n",
    "            if i not in inverted_index[token]:\n",
    "                inverted_index[token].append(i)\n",
    "    return dict(inverted_index)\n",
    "\n",
    "def store_inverted_index(inverted_index, file_name):\n",
    "    json_data = json.dumps(inverted_index)\n",
    "    with gzip.open(file_name, 'wt') as f:\n",
    "        f.write(json_data) \n",
    "\n",
    "def process_query(query):\n",
    "    query_array = []\n",
    "    query = remove_whitespace(query)\n",
    "    query = text_lowercase(query)\n",
    "    query = process_shortcuts(query)\n",
    "    query = format_dates(query)\n",
    "    # query = error_detection(query)\n",
    "    words = word_tokenize(query)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = remove_lemma(words)\n",
    "    words = stem_words(words)\n",
    "    query_array = words\n",
    "    return query_array\n",
    "\n",
    "def check_word_exist_in_doc(word, docs_array):\n",
    "    for doc in docs_array:\n",
    "        for term in doc:\n",
    "            if word == term:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def expand_query(query_array, docs_array):\n",
    "    synonyms = []\n",
    "    for word in query_array:\n",
    "        word_synonyms = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if check_word_exist_in_doc(lemma.name(), docs_array):\n",
    "                        word_synonyms.add(lemma.name())\n",
    "        word_synonyms = sorted(word_synonyms, key=lambda x: nltk.edit_distance(word, x))[:3]\n",
    "        synonyms.append(word_synonyms)\n",
    "    \n",
    "    new_synonyms = []\n",
    "    for syn in synonyms:\n",
    "        for term in syn:\n",
    "            if syn not in query_array:\n",
    "                new_synonyms.append(term)\n",
    "    expand_query = \" \".join(query_array)+\" \"+\" \".join(new_synonyms)\n",
    "    return expand_query\n",
    "\n",
    "\n",
    "def make_tf_idf_values(docs_array):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in docs_array])\n",
    "    return vectorizer.get_feature_names_out(), tfidf_matrix, vectorizer\n",
    "\n",
    "def load_vect_and_tfidf(tfidf_file, vectorizer_file):\n",
    "    tfidf_matrix = load_npz(tfidf_file)\n",
    "    with open(vectorizer_file, 'rb') as f:\n",
    "        vectorizer = pickle.load(f)\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "\n",
    "def get_query_result(query_array, vectorizer, tfidf, expand_query):\n",
    "    q = \" \".join(query_array)\n",
    "    ex_q = \" \".join(expand_query)\n",
    "    \n",
    "    query_vec = vectorizer.transform([q])\n",
    "    expanded_query_vector = vectorizer.transform([ex_q])\n",
    "    \n",
    "    alpha = 0.7\n",
    "    beta = 0.3\n",
    "    \n",
    "    weighted_query_vector = alpha * query_vec + beta * expanded_query_vector\n",
    "    cosine_similarities = cosine_similarity(weighted_query_vector, tfidf)\n",
    "    sorted_doc_indices = np.argsort(cosine_similarities[0])[-10:]\n",
    "    sorted_scores = cosine_similarities[0][sorted_doc_indices]\n",
    "    return sorted_doc_indices, sorted_scores\n",
    "\n",
    "def get_documents(sorted_doc_indices, sorted_scores, dataset_type):\n",
    "    ranked_doc = []\n",
    "    cnt = len(sorted_scores)-1\n",
    "    for idx in sorted_doc_indices:\n",
    "        if dataset_type == 1:\n",
    "            element = {\n",
    "                \"document\": documents[idx],\n",
    "                \"socre\": '{:.3f}'.format(sorted_scores[cnt]),\n",
    "                \"index\": documents_keys[idx]\n",
    "            }\n",
    "        else:\n",
    "             element = {\n",
    "                \"document\": documents2[idx],\n",
    "                \"socre\": '{:.3f}'.format(sorted_scores[cnt]),\n",
    "                \"index\": documents_keys2[idx]\n",
    "            }\n",
    "        cnt-=1\n",
    "        ranked_doc.append(element)\n",
    "    return ranked_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign_data_set_to_documents() \n",
    "# docs_array = text_processing(documents)\n",
    "\n",
    "assign_data_set_to_documents2()\n",
    "# docs_array2 = text_processing(documents2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def store_docs_array_in_file(docs_array, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(docs_array)\n",
    "def read_docs_array_from_file(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        docs_array = []\n",
    "        for row in reader:\n",
    "            docs_array.append(row)\n",
    "    return docs_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_array2 = read_docs_array_from_file('docs_array2.csv')\n",
    "# docs_array = read_docs_array_from_file('docs_array.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = make_inverted_index(docs_array2)\n",
    "store_inverted_index(inverted_index, 'inverted_index2.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names, tfidf_matrix2, vectorizer2 = make_tf_idf_values(docs_array2)\n",
    "def store_tfidf_vecotrizer(tfidf_matrix_file_name,vectirizer_file_name , tfidf, vecotrizer):\n",
    "    save_npz(tfidf_matrix_file_name, tfidf)\n",
    "    with open(vectirizer_file_name, 'wb') as f:\n",
    "        pickle.dump(vecotrizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix2, vectorizer2 = load_vect_and_tfidf(\"tfidf_matrix2.npz\", \"vectorizer2.pkl\")\n",
    "# tfidf_matrix, vectorizer = load_vect_and_tfidf(\"tfidf_matrix.npz\", \"vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Jun/2023 10:34:36] \"POST /search HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jun/2023 10:34:36] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [08/Jun/2023 10:34:55] \"POST /search HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/search', methods=['POST'])\n",
    "\n",
    "def searchText():\n",
    "    \n",
    "    query = request.form['search']\n",
    "    dataset = request.form['dataset']\n",
    "    query_array = process_query(query)\n",
    "    #expand the query\n",
    "    \n",
    "    sorted_docs_with_scores = []\n",
    "    \n",
    "    if dataset == 'dataset1':\n",
    "        # antique dataset\n",
    "        expanded_query = expand_query(query_array, docs_array)\n",
    "        sorted_doc_indices, sorted_scores = get_query_result(query_array, vectorizer, tfidf_matrix, expanded_query)\n",
    "        sorted_docs_with_scores = get_documents(sorted_doc_indices, sorted_scores, 1)\n",
    "        \n",
    "    else:   # dataset2\n",
    "        expanded_query = expand_query(query_array, docs_array2)\n",
    "        sorted_doc_indices, sorted_scores = get_query_result(query_array, vectorizer2, tfidf_matrix2, expanded_query)\n",
    "        sorted_docs_with_scores = get_documents(sorted_doc_indices, sorted_scores, 2)\n",
    "        \n",
    "    return render_template('results.html', results=sorted_docs_with_scores)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = []\n",
    "queries2 = []\n",
    "\n",
    "def get_queries():\n",
    "    global queries\n",
    "    counter = 0\n",
    "    for query in dataset.queries_iter():\n",
    "        if counter >=50:\n",
    "            break\n",
    "        counter+=1\n",
    "        queries.append(query)\n",
    "        \n",
    "\n",
    "def get_queries2():\n",
    "    global queries2\n",
    "    counter = 0\n",
    "    for query in dataset2.queries_iter():\n",
    "        if counter >=50:\n",
    "            break\n",
    "        counter+=1\n",
    "        queries2.append(query)\n",
    "    \n",
    "        \n",
    "qrels = []\n",
    "qrels2 = []\n",
    "\n",
    "def get_qrels():\n",
    "    for qrel in dataset.qrels_iter():\n",
    "         query_id = qrel[0]\n",
    "         if query_id in [query[0] for query in queries]:\n",
    "            qrels.append(qrel)\n",
    "        \n",
    "def get_qrels2():\n",
    "    for qrel in dataset2.qrels_iter():\n",
    "        query_id = qrel[0]\n",
    "        if query_id in [query[0] for query in queries2]:\n",
    "            qrels2.append(qrel)\n",
    "\n",
    "retrieved_docs = {}\n",
    "retrieved_docs2 = {}\n",
    "def get_retrieved_documents(datasets_type):\n",
    "    if datasets_type==1:\n",
    "        for query in queries:\n",
    "            q = query.text\n",
    "            retrieved_docs[query.query_id] = {}\n",
    "            query_array = process_query(q)\n",
    "            expanded_query = expand_query(query_array, docs_array)\n",
    "            sorted_doc_indices, sorted_scores = get_query_result(query_array, vectorizer, tfidf_matrix, expanded_query)\n",
    "            sorted_docs_with_scores = get_documents(sorted_doc_indices, sorted_scores, 1)\n",
    "            retrieved_docs[query.query_id]  = sorted_docs_with_scores\n",
    "    else:\n",
    "        for query in queries2:\n",
    "            q = query.text\n",
    "            retrieved_docs2[query.query_id] = {}\n",
    "            query_array = process_query(q)\n",
    "            expanded_query = expand_query(query_array, docs_array2)\n",
    "            sorted_doc_indices, sorted_scores = get_query_result(query_array, vectorizer2, tfidf_matrix2, expanded_query)\n",
    "            sorted_docs_with_scores = get_documents(sorted_doc_indices, sorted_scores, 2)\n",
    "            retrieved_docs2[query.query_id]  = sorted_docs_with_scores\n",
    "        \n",
    "def get_QRELS(datasets_type):\n",
    "    retrievedRelevantDocs = {}\n",
    "    if datasets_type==1:\n",
    "        for qrel in qrels:\n",
    "            if qrel.query_id not in retrievedRelevantDocs:\n",
    "                retrievedRelevantDocs[qrel.query_id] = [] \n",
    "            retrievedRelevantDocs[qrel.query_id].append({'relevance': qrel.relevance, 'doc_id':qrel.doc_id})\n",
    "    else:\n",
    "        for qrel in qrels2:\n",
    "            if qrel.query_id not in retrievedRelevantDocs:\n",
    "                retrievedRelevantDocs[qrel.query_id] = [] \n",
    "            retrievedRelevantDocs[qrel.query_id].append({'relevance': qrel.relevance, 'doc_id':qrel.doc_id})\n",
    "    return retrievedRelevantDocs\n",
    "\n",
    "def get_relevance_non_relevance_docs(relevantDocs, datasets_type):\n",
    "    qrels = {}\n",
    "    threshold = 1\n",
    "    for qrel in relevantDocs:\n",
    "        # Retrieve the query ID and relevant documents\n",
    "        query_id = qrel\n",
    "        relevant_docs = relevantDocs[query_id]\n",
    "        # Add the query ID to the qrels dictionary\n",
    "        qrels[query_id] = []\n",
    "        # Iterate over all the documents in the dataset and add them to the qrels dictionary\n",
    "        if datasets_type==1:\n",
    "            for doc_id in documents_keys:\n",
    "                relevance = 2\n",
    "                if doc_id in [d['doc_id'] for  d in relevant_docs]:\n",
    "                    relevance = 1\n",
    "                else:\n",
    "                    relevance = 0\n",
    "\n",
    "                # Add the document and relevance score to the qrels dictionary\n",
    "                qrels[query_id].append({'doc_id':doc_id, 'relevance':relevance})\n",
    "        else:\n",
    "            for doc_id in documents_keys2:\n",
    "                relevance = 2\n",
    "                if doc_id in [d['doc_id'] for  d in relevant_docs]:\n",
    "                        relevance = 1\n",
    "                else:\n",
    "                        relevance = 0\n",
    "\n",
    "                # Add the document and relevance score to the qrels dictionary\n",
    "                qrels[query_id].append({'doc_id':doc_id, 'relevance':relevance})\n",
    "    return qrels\n",
    "\n",
    "def getRelevance1(query_id, qrels_new):\n",
    "    relevance1 = set()\n",
    "    for doc in qrels_new.get(query_id):\n",
    "        if(doc['relevance']==1):\n",
    "            relevance1.add(doc['doc_id'])\n",
    "    return relevance1\n",
    "\n",
    "def getRetrievedDocs(retrieved):\n",
    "    retrievedDocs = set()\n",
    "    for doc in retrieved:\n",
    "        retrievedDocs.add(doc['index'])\n",
    "    return retrievedDocs\n",
    "\n",
    "def precission_at_10(relevance, retrieved):\n",
    "    num_relevant_retrieved = len(set(relevance).intersection(retrieved))\n",
    "    precision_at_10 = num_relevant_retrieved / 10\n",
    "    return precision_at_10\n",
    "\n",
    "def recall_values(relevance, retrieved):\n",
    "    num_relevant_retrieved = len(relevance.intersection(retrieved))\n",
    "    num_relevant_total = len(relevance)\n",
    "    recall = num_relevant_retrieved / num_relevant_total\n",
    "    return recall\n",
    "\n",
    "def mean_avg_precision(relevance1, retrieved):\n",
    "    precision_sum = 0.0\n",
    "    num_relevant = len(relevance1)\n",
    "    num_correct = 0\n",
    "    for i, doc in enumerate(retrieved):\n",
    "        if doc in relevance1:\n",
    "            num_correct += 1\n",
    "            precision = num_correct / (i + 1)\n",
    "            precision_sum += precision\n",
    "\n",
    "    ap  = precision_sum / num_relevant\n",
    "    return ap\n",
    "\n",
    "def mean_reciprocal_rank(relevance1, retrieved):\n",
    "    rr = 0\n",
    "    for i, doc in enumerate(retrieved):\n",
    "        if doc in relevance1:\n",
    "            rr = 1/(i+1)\n",
    "            break\n",
    "    return rr\n",
    "\n",
    "\n",
    "def calc_evaluation(qrels_new):\n",
    "    AP = []\n",
    "    MRR = []\n",
    "    \n",
    "    for query in queries:\n",
    "        \n",
    "        relevance1 = getRelevance1(query.query_id, qrels_new)\n",
    "        retrieved = getRetrievedDocs(retrieved_docs[query.query_id])\n",
    "        #recall\n",
    "        r = recall_values(relevance1, retrieved)\n",
    "    #     #precission @ 10\n",
    "        p = precission_at_10(relevance1, retrieved)\n",
    "        with open('evaluation.txt', 'a') as f:\n",
    "            f.write(f\"{query.query_id}: precision@k:{p:.3f} recall:{r:.3f}\\n\")\n",
    "        \n",
    "        ap = mean_avg_precision(relevance1, retrieved)\n",
    "        AP.append(ap)\n",
    "        \n",
    "        rr = mean_reciprocal_rank(relevance1, retrieved)\n",
    "        MRR.append(rr)\n",
    "    #MRR\n",
    "    mean_MRR = sum(MRR) / len(MRR)\n",
    "    #MAP\n",
    "    MAP = sum(AP) / len(AP)\n",
    "    with open('evaluation.txt', 'a') as f:\n",
    "        f.write(f\"{query.query_id}: MRR:{mean_MRR:.3f} MAP:{MAP:.3f}\\n\")\n",
    "    \n",
    "def calc_evaluation2(qrels_new):\n",
    "    AP = []\n",
    "    MRR = []\n",
    "    \n",
    "    for query in queries2:\n",
    "        \n",
    "        relevance1 = getRelevance1(query.query_id, qrels_new)\n",
    "        retrieved = getRetrievedDocs(retrieved_docs2[query.query_id])\n",
    "        #recall\n",
    "        r = recall_values(relevance1, retrieved)\n",
    "    #     #precission @ 10\n",
    "        p = precission_at_10(relevance1, retrieved)\n",
    "        with open('evaluation2.txt', 'a') as f:\n",
    "            f.write(f\"{query.query_id}: precision@k:{p:.3f} recall:{r:.3f}\\n\")\n",
    "        \n",
    "        ap = mean_avg_precision(relevance1, retrieved)\n",
    "        AP.append(ap)\n",
    "        \n",
    "        rr = mean_reciprocal_rank(relevance1, retrieved)\n",
    "        MRR.append(rr)\n",
    "    #MRR\n",
    "    mean_MRR = sum(MRR) / len(MRR)\n",
    "    #MAP\n",
    "    MAP = sum(AP) / len(AP)\n",
    "    with open('evaluation2.txt', 'a') as f:\n",
    "        f.write(f\"{query.query_id}: MRR:{mean_MRR:.3f} MAP:{MAP:.3f}\\n\")\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_queries()\n",
    "# get_qrels()\n",
    "get_queries2()\n",
    "get_qrels2()\n",
    "get_retrieved_documents(2)\n",
    "relevantDocs = get_QRELS(2)\n",
    "qrels_new = get_relevance_non_relevance_docs(relevantDocs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_evaluation2(qrels_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\fifth year\\فصل تاني\\ir\\ir project\\jup.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_retrieved_documents(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m relevantDocs \u001b[39m=\u001b[39m get_QRELS(\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qrels_new \u001b[39m=\u001b[39m get_relevance_non_relevance_docs(relevantDocs, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32me:\\fifth year\\فصل تاني\\ir\\ir project\\jup.ipynb Cell 25\u001b[0m in \u001b[0;36mget_retrieved_documents\u001b[1;34m(datasets_type)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m query_array \u001b[39m=\u001b[39m process_query(q)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m expanded_query \u001b[39m=\u001b[39m expand_query(query_array, docs_array2)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m sorted_doc_indices, sorted_scores \u001b[39m=\u001b[39m get_query_result(query_array, vectorizer2, tfidf_matrix2, expanded_query)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m sorted_docs_with_scores \u001b[39m=\u001b[39m get_documents(sorted_doc_indices, sorted_scores, \u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m retrieved_docs2[query\u001b[39m.\u001b[39mquery_id]  \u001b[39m=\u001b[39m sorted_docs_with_scores\n",
      "\u001b[1;32me:\\fifth year\\فصل تاني\\ir\\ir project\\jup.ipynb Cell 25\u001b[0m in \u001b[0;36mget_query_result\u001b[1;34m(query_array, vectorizer, tfidf, expand_query)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39m0.3\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=273'>274</a>\u001b[0m weighted_query_vector \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m query_vec \u001b[39m+\u001b[39m beta \u001b[39m*\u001b[39m expanded_query_vector\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=274'>275</a>\u001b[0m cosine_similarities \u001b[39m=\u001b[39m cosine_similarity(weighted_query_vector, tfidf)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=275'>276</a>\u001b[0m sorted_doc_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(cosine_similarities[\u001b[39m0\u001b[39m])[\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m:]\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/fifth%20year/%D9%81%D8%B5%D9%84%20%D8%AA%D8%A7%D9%86%D9%8A/ir/ir%20project/jup.ipynb#X33sZmlsZQ%3D%3D?line=276'>277</a>\u001b[0m sorted_scores \u001b[39m=\u001b[39m cosine_similarities[\u001b[39m0\u001b[39m][sorted_doc_indices]\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1401\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1399\u001b[0m     Y_normalized \u001b[39m=\u001b[39m normalize(Y, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m-> 1401\u001b[0m K \u001b[39m=\u001b[39m safe_sparse_dot(X_normalized, Y_normalized\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49mdense_output)\n\u001b[0;32m   1403\u001b[0m \u001b[39mreturn\u001b[39;00m K\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:541\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m other\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    540\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mdimension mismatch\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 541\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_sparse_matrix(other)\n\u001b[0;32m    543\u001b[0m \u001b[39m# If it's a list or whatever, treat it like a matrix\u001b[39;00m\n\u001b[0;32m    544\u001b[0m other_a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masanyarray(other)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:512\u001b[0m, in \u001b[0;36m_cs_matrix._mul_sparse_matrix\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    509\u001b[0m K2, N \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39mshape\n\u001b[0;32m    511\u001b[0m major_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap((M, N))[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 512\u001b[0m other \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m(other)  \u001b[39m# convert to this format\u001b[39;00m\n\u001b[0;32m    514\u001b[0m idx_dtype \u001b[39m=\u001b[39m get_index_dtype((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindptr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices,\n\u001b[0;32m    515\u001b[0m                              other\u001b[39m.\u001b[39mindptr, other\u001b[39m.\u001b[39mindices))\n\u001b[0;32m    517\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matmat_maxnnz\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:33\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     31\u001b[0m         arg1 \u001b[39m=\u001b[39m arg1\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     32\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         arg1 \u001b[39m=\u001b[39m arg1\u001b[39m.\u001b[39;49masformat(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat)\n\u001b[0;32m     34\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_self(arg1)\n\u001b[0;32m     36\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(arg1, \u001b[39mtuple\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_base.py:376\u001b[0m, in \u001b[0;36mspmatrix.asformat\u001b[1;34m(self, format, copy)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39m# Forward the copy kwarg, if it's accepted.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_method(copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    377\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    378\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_method()\n",
      "File \u001b[1;32mc:\\Users\\Dell\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\sparse\\_csc.py:140\u001b[0m, in \u001b[0;36mcsc_matrix.tocsr\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    137\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnz, dtype\u001b[39m=\u001b[39midx_dtype)\n\u001b[0;32m    138\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnnz, dtype\u001b[39m=\u001b[39mupcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 140\u001b[0m csc_tocsr(M, N,\n\u001b[0;32m    141\u001b[0m           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr\u001b[39m.\u001b[39;49mastype(idx_dtype),\n\u001b[0;32m    142\u001b[0m           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices\u001b[39m.\u001b[39;49mastype(idx_dtype),\n\u001b[0;32m    143\u001b[0m           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[0;32m    144\u001b[0m           indptr,\n\u001b[0;32m    145\u001b[0m           indices,\n\u001b[0;32m    146\u001b[0m           data)\n\u001b[0;32m    148\u001b[0m A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_csr_container(\n\u001b[0;32m    149\u001b[0m     (data, indices, indptr),\n\u001b[0;32m    150\u001b[0m     shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    151\u001b[0m )\n\u001b[0;32m    152\u001b[0m A\u001b[39m.\u001b[39mhas_sorted_indices \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_retrieved_documents(2)\n",
    "relevantDocs = get_QRELS(2)\n",
    "qrels_new = get_relevance_non_relevance_docs(relevantDocs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_evaluation2(qrels_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
